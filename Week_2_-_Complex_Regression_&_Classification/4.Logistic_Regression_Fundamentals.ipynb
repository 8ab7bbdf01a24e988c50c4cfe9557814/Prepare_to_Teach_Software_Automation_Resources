{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Dependancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils_common import dlc, plot_data, draw_vthresh, compute_model_output\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.style.use('./deeplearning.mplstyle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can we use Linear Regression for classification?\n",
    "correlation = 0.8       # Correlation between -1 and 1\n",
    "n = 300                 # Sample\n",
    "\n",
    "xx = np.array([-10, 2.5])\n",
    "yy = np.array([0, 0])\n",
    "means = [xx.mean(), yy.mean()]  \n",
    "stds = [xx.std() / 3, yy.std() / 3]\n",
    "covs = [[stds[0]**2, stds[0]*stds[1]*correlation], \n",
    "[stds[0]*stds[1]*correlation, stds[1]**2]] \n",
    "o = np.random.multivariate_normal(means, covs, n).T\n",
    "\n",
    "xx = np.array([-2.5, 10])\n",
    "yy = np.array([1, 1])\n",
    "means = [xx.mean(), yy.mean()]  \n",
    "stds = [xx.std() / 3, yy.std() / 3]\n",
    "covs = [[stds[0]**2, stds[0]*stds[1]*correlation], \n",
    "[stds[0]*stds[1]*correlation, stds[1]**2]] \n",
    "n = np.random.multivariate_normal(means, covs, n).T\n",
    "\n",
    "m = 0.05\n",
    "b = 0.5\n",
    "# Anything close to these values is acceptable\n",
    "\n",
    "tmp_f_mb = compute_model_output(np.array([-10, 10]), m, b,)\n",
    "\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"Target\")\n",
    "plt.scatter(o[0], o[1], color='red')\n",
    "plt.scatter(n[0], n[1], color='blue')\n",
    "plt.plot(np.array([-10, 10]), tmp_f_mb, c='g',label='Our Prediction')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise classification\n",
    "x_train = np.array([-3, -4, -7, -9, 3, 4, 7, 8])\n",
    "y_train = np.array([0, 0, 0, 0, 1, 1, 1, 1])\n",
    "X_train2 = np.array([[0.5, 1.5], [1, 1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])\n",
    "y_train2 = np.array([0, 0, 0, 1, 1, 1])\n",
    "\n",
    "pos = y_train == 1\n",
    "neg = y_train == 0\n",
    "\n",
    "fig,ax = plt.subplots(1,2,figsize=(8,3))\n",
    "#plot 1, single variable\n",
    "ax[0].scatter(x_train[pos], y_train[pos], marker='x', s=80, c = 'red', label=\"y=1\")\n",
    "ax[0].scatter(x_train[neg], y_train[neg], marker='o', s=100, label=\"y=0\", facecolors='none', edgecolors=dlc[\"dlblue\"],lw=3)\n",
    "\n",
    "ax[0].set_ylim(-0.08,1.1)\n",
    "ax[0].set_ylabel('y', fontsize=12)\n",
    "ax[0].set_xlabel('x', fontsize=12)\n",
    "ax[0].set_title('Single feature plot')\n",
    "ax[0].legend()\n",
    "\n",
    "#plot 2, two variables\n",
    "plot_data(X_train2, y_train2, ax[1] )\n",
    "ax[1].axis([0, 4, 0, 4])\n",
    "ax[1].set_ylabel('$x_1$', fontsize=12)\n",
    "ax[1].set_xlabel('$x_0$', fontsize=12)\n",
    "ax[1].set_title('Two feature plot')\n",
    "ax[1].legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple logistic function\n",
    "\n",
    "<span style=\"color : red\">Students are not expected to recall or be able to \n",
    "calculate this function</span>\n",
    "\n",
    "$f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = g(\\mathbf{w} \\cdot  \\mathbf{x}^{(i)} + b)$, to predict $y$ given $x$\n",
    "\n",
    "| Symbol | |\n",
    "| --- | --- |\n",
    "| $f$ | A function indicating a rule that assigns a unique output value (y) for each input value (x) |\n",
    "| $w$ | Weight is the importance or influence of each feature on the prediction |\n",
    "| $b$ | Bias is a constant that shifts the decision boundary |\n",
    "| $x$ | Feature(s) |\n",
    "| $g$ | The sigmoid function and it maps all input values to values between 0 and 1 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid Function & Decision Boundary\n",
    "def sigmoid(z):\n",
    "    g = 1/(1+np.exp(-z))\n",
    "    return g\n",
    "z_tmp = np.arange(-10,10)\n",
    "y = sigmoid(z_tmp)\n",
    "\n",
    "fig,ax = plt.subplots(1,1,figsize=(5,3))\n",
    "ax.plot(z_tmp, y, c=\"b\")\n",
    "\n",
    "ax.set_title(\"Sigmoid Function and decision boundary for one feature\")\n",
    "ax.set_ylabel('Target')\n",
    "ax.set_xlabel('Feature')\n",
    "\n",
    "#ax.scatter(n[0], n[1], color='blue')\n",
    "ax.grid(True)\n",
    "draw_vthresh(ax,0)\n",
    "ax.scatter(x_train, y_train, s=100, c=y_train, zorder=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use the sigmoid function to find the decision boundary\n",
    "# The decision boundary is the line where the output probability is 0.5\n",
    "\n",
    "\n",
    "x0 = np.arange(0,6)\n",
    "x1 = 3 - x0\n",
    "fig,ax = plt.subplots(1,1,figsize=(5,4))\n",
    "ax.set_title(\"Decision boundary for two features\")\n",
    "ax.plot(x0,x1, c=\"b\")\n",
    "ax.axis([0, 4, 0, 3.5])\n",
    "ax.fill_between(x0,x1, alpha=0.2)\n",
    "plot_data(X_train2, y_train2,ax)\n",
    "ax.set_ylabel(r'$x_1$')\n",
    "ax.set_xlabel(r'$x_0$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now you have a sigmoid curve that defines a decision boundary, what do you think are the next steps in the Logistic Regression algorythm?\n",
    "\n",
    "<details>\n",
    "    <summary><h3 style=\"display:inline\">I want to some scary math</h3></summary>\n",
    "\n",
    "#### Logistic Cost Function\n",
    "\n",
    "<span style=\"color : red\">Students are not expected to recall or be able to \n",
    "calculate these functions</span>\n",
    "\n",
    "$$ J(\\mathbf{w},b) = \\frac{1}{m} \\sum_{i=0}^{m-1} \\left[ loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) \\right] $$\n",
    "\n",
    "#### Gradient Decent\n",
    "\n",
    "$$\\begin{align*}\n",
    "&\\text{repeat until convergence:} \\; \\lbrace \\\\\n",
    "&  \\; \\; \\;w_j = w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}   \\; & \\text{for j := 0..n-1} \\\\ \n",
    "&  \\; \\; \\;  \\; \\;b = b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b} \\\\\n",
    "&\\rbrace\n",
    "\\end{align*}$$\n",
    "\n",
    "</details>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
